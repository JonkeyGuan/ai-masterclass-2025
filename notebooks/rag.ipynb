{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c31c3e8",
   "metadata": {},
   "source": [
    "# RAG from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cdbbf",
   "metadata": {},
   "source": [
    "- Adapted from https://github.com/opendatahub-io/llama-stack-demos/\n",
    "- Requires distributions/masterclass-agents/run.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8319431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:8321\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9d0689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='ollama', provider_resource_id='all-minilm:latest', type='model', model_type='embedding'),\n",
       " Model(identifier='meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='ollama', provider_resource_id='llama3.2:3b-instruct-fp16', type='model', model_type='llm')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d21e1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='my-vector-db', provider_id='milvus', provider_resource_id='my-vector-db', type='vector_db', access_attributes=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_id = \"my-vector-db\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf72cc",
   "metadata": {},
   "source": [
    "Ingesting documents into a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9502ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaf4f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-0'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-1'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-2'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2066200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c68396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the top 5 topics that were explained? Only list succinct bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2328bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryResult</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span><span style=\"font-weight: bold\">]}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 1:\\nDocument_id:num-3\\nContent: ,\\n    and (b) the memory constraints of your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 2:\\nDocument_id:num-0\\nContent:  Instruct.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates &amp; special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n            \"role\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n        {\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        },\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        },\\n    ]\\n\\nNow, let\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 3:\\nDocument_id:num-2\\nContent: ora_finetune_single_device --config </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama3/8B_qlora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=32 \\\\\\n  model.lora_alpha=64\\n\\n\\nor, by</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.qlora_llama3_8b\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 32\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(DoRA)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;https://arxiv.org/abs/2402.09353&gt;`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">How do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``use_dora=True``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 4:\\nDocument_id:num-2\\nContent: _lora_single_device  \\\\\\n  model.apply_lora_to_mlp=True </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"]\\n\\n.. code-block:: yaml\\n\\n  model:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: torchtune.models.llama3.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    model.lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\"q_proj\", \"k_proj\", \"v_proj\",\"output_proj\"]\\n\\nSecondly, parameters which control the scale of the impact of LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on the model:\\n\\n* ``lora_rank: int`` affects the scale of the LoRA decomposition, where ``lora_rank &lt;&lt; in_dim`` </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and ``lora_rank &lt;&lt; out_dim``\\n  \\\\- the dimensions of an arbitrary linear layer in the model. Concretely, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``lora_rank`` reduces the number of gradients stored\\n  in a linear fashion from ``in_dim * out_dim`` to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``lora_rank * (in_dim + out_dim)``. Typically, we have ``lora_rank in [8, 256]``.\\n* ``lora_alpha: float`` affects </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the magnitude of the LoRA updates. A larger alpha results in larger updates to the base model weights\\n  , </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">potentially at the cost of training stability, conversely, smaller alpha can stabilize training at the cost of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slower learning.\\n  We provide default settings for these parameters which we\\'ve tested with all of our models, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">but we encourage you to adjust them\\n  to your specific use case. Typically, one jointly changes ``lora_rank`` and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``lora_alpha`` together, where ``lora_alpha ~= 2*lora_rank``.\\n* ``lora_dropout`` introduces dropout in the LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layers to help regularize training. We default to 0.0 for all of our models.\\n\\nAs above, these parameters are also</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specified under the ``model`` flag or config entry:\\n\\n.. code-block:: bash\\n\\n  tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_finetune_single_device --config llama3/8B_lora_single_device  \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"] \\\\\\n  model.lora_rank=32 \\\\\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.lora_alpha=64\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.llama3.lora_llama3_8b\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">apply_lora_to_mlp: True\\n    l\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 5:\\nDocument_id:num-2\\nContent: wd`\", \"Use it when you have large gradients and can fit a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large enough batch size, since this is not compatible with ``gradient_accumulation_steps``.\"\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`glossary_cpu_offload`\", \"Offloads optimizer states and (optionally) gradients to CPU, and performs optimizer</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">steps on CPU. This can be used to significantly reduce GPU memory usage at the cost of CPU RAM and training speed. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Prioritize using it only if the other techniques are not enough.\"\\n   \":ref:`glossary_lora`\", \"When you want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly reduce the number of trainable parameters, saving gradient and optimizer memory during training, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly speeding up training. This may reduce training accuracy\"\\n   \":ref:`glossary_qlora`\", \"When you are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training a large model, since quantization will save 1.5 bytes * (# of model parameters), at the potential cost of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">some training speed and accuracy.\"\\n   \":ref:`glossary_dora`\", \"a variant of LoRA that may improve model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance at the cost of slightly more memory.\"\\n\\n\\n.. note::\\n\\n  In its current state, this tutorial is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">focused on single-device optimizations. Check in soon as we update this page\\n  for the latest memory optimization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">features for distributed fine-tuning.\\n\\n.. _glossary_precision:\\n\\n\\nModel Precision\\n---------------\\n\\n*What\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">going on here?*\\n\\nWe use the term \"precision\" to refer to the underlying data type used to represent the model and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimizer parameters.\\nWe support two data types in torchtune:\\n\\n.. note::\\n\\n  We recommend diving into Sebastian</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Raschka\\'s `blogpost on mixed-precision techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html&gt;`_\\n  for a deeper understanding of concepts </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around precision and data formats.\\n\\n* ``fp32``, commonly referred to as \"full-precision\", uses 4 bytes per model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and optimizer parameter.\\n* ``bfloat16``, referred to as \"half-precision\", uses 2 bytes per model and optimizer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameter - effectively half\\n  the memory of ``fp32``, and also improves training speed. Generally, if your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware supports training with ``bfloat16``,\\n  we recommend using it - this is the default setting for our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recipes.\\n\\n.. note::\\n\\n  Another common paradigm is \"mixed-precision\" training: where model weights are in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``bfloat16`` (or ``fp16``), and optimizer\\n  states are in ``fp32``. Currently,\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'END of knowledge_search tool results.\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'num-3'\u001b[0m, \u001b[32m'num-0'\u001b[0m, \u001b[32m'num-2'\u001b[0m, \u001b[32m'num-2'\u001b[0m, \u001b[32m'num-2'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 1:\\nDocument_id:num-3\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your \u001b[0m\n",
       "\u001b[32mhardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to \u001b[0m\n",
       "\u001b[32mexperiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA \u001b[0m\n",
       "\u001b[32mto Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply \u001b[0m\n",
       "\u001b[32mLoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to\u001b[0m\n",
       "\u001b[32mincrease our max memory,\\nbut as long as we keep :code:`rank\u001b[0m\u001b[32m<\u001b[0m\u001b[32m<embed_dim`, the impact should be relatively \u001b[0m\n",
       "\u001b[32mminor.\\n\\nLet\\'s run this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and\u001b[0m\n",
       "\u001b[32mrank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \u001b[0m\n",
       "\u001b[32m--config llama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between \u001b[0m\n",
       "\u001b[32mthis run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 2:\\nDocument_id:num-0\\nContent:  Instruct.\\n\\n.. \u001b[0m\n",
       "\u001b[32m_prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates & special \u001b[0m\n",
       "\u001b[32mtokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn \u001b[0m\n",
       "\u001b[32maccompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \u001b[0m\n",
       "\u001b[32m\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n \u001b[0m\n",
       "\u001b[32m\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop\u001b[0m\n",
       "\u001b[32m\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s \u001b[0m\n",
       "\u001b[32mformat this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The\u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to \u001b[0m\n",
       "\u001b[32mindicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, \u001b[0m\n",
       "\u001b[32mMessage\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m <<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 3:\\nDocument_id:num-2\\nContent: ora_finetune_single_device --config \u001b[0m\n",
       "\u001b[32mllama3/8B_qlora_single_device \\\\\\n  model.\u001b[0m\u001b[32mapply_lora_to_mlp\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \\\\\\n  \u001b[0m\n",
       "\u001b[32mmodel.\u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\",\"k_proj\",\"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m\\n\\n\\nor, by\u001b[0m\n",
       "\u001b[32mmodifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.qlora_llama3_8b\\n    \u001b[0m\n",
       "\u001b[32mapply_lora_to_mlp: True\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\", \"k_proj\", \"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 32\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mDoRA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA \u001b[0m\n",
       "\u001b[32m<https://arxiv.org/abs/2402.09353>`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing \u001b[0m\n",
       "\u001b[32mthe pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector \u001b[0m\n",
       "\u001b[32mthat adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates \u001b[0m\n",
       "\u001b[32mthe orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude \u001b[0m\n",
       "\u001b[32mparameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! \u001b[0m\n",
       "\u001b[32mHow do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the\u001b[0m\n",
       "\u001b[32msame model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with \u001b[0m\n",
       "\u001b[32m``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use \u001b[0m\n",
       "\u001b[32m:func:`torchtune.models.llama3.lora_llama3_8b` with ``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``:\\n\\n.. code-block:: bash\\n\\n  tune run \u001b[0m\n",
       "\u001b[32mlora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends \u001b[0m\n",
       "\u001b[32mLoRA\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 4:\\nDocument_id:num-2\\nContent: _lora_single_device  \\\\\\n  model.\u001b[0m\u001b[32mapply_lora_to_mlp\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m\\\\\\n  model.\u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n.. code-block:: yaml\\n\\n  model:\\n    \u001b[0m\n",
       "\u001b[32m_component_: torchtune.models.llama3.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    model.lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\"q_proj\", \"k_proj\", \"v_proj\",\"output_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nSecondly, parameters which control the scale of the impact of LoRA \u001b[0m\n",
       "\u001b[32mon the model:\\n\\n* ``lora_rank: int`` affects the scale of the LoRA decomposition, where ``lora_rank << in_dim`` \u001b[0m\n",
       "\u001b[32mand ``lora_rank << out_dim``\\n  \\\\- the dimensions of an arbitrary linear layer in the model. Concretely, \u001b[0m\n",
       "\u001b[32m``lora_rank`` reduces the number of gradients stored\\n  in a linear fashion from ``in_dim * out_dim`` to \u001b[0m\n",
       "\u001b[32m``lora_rank * \u001b[0m\u001b[32m(\u001b[0m\u001b[32min_dim + out_dim\u001b[0m\u001b[32m)\u001b[0m\u001b[32m``. Typically, we have ``lora_rank in \u001b[0m\u001b[32m[\u001b[0m\u001b[32m8, 256\u001b[0m\u001b[32m]\u001b[0m\u001b[32m``.\\n* ``lora_alpha: float`` affects \u001b[0m\n",
       "\u001b[32mthe magnitude of the LoRA updates. A larger alpha results in larger updates to the base model weights\\n  , \u001b[0m\n",
       "\u001b[32mpotentially at the cost of training stability, conversely, smaller alpha can stabilize training at the cost of \u001b[0m\n",
       "\u001b[32mslower learning.\\n  We provide default settings for these parameters which we\\'ve tested with all of our models, \u001b[0m\n",
       "\u001b[32mbut we encourage you to adjust them\\n  to your specific use case. Typically, one jointly changes ``lora_rank`` and \u001b[0m\n",
       "\u001b[32m``lora_alpha`` together, where ``lora_alpha ~= 2*lora_rank``.\\n* ``lora_dropout`` introduces dropout in the LoRA \u001b[0m\n",
       "\u001b[32mlayers to help regularize training. We default to 0.0 for all of our models.\\n\\nAs above, these parameters are also\u001b[0m\n",
       "\u001b[32mspecified under the ``model`` flag or config entry:\\n\\n.. code-block:: bash\\n\\n  tune run \u001b[0m\n",
       "\u001b[32mlora_finetune_single_device --config llama3/8B_lora_single_device  \\\\\\n  model.\u001b[0m\u001b[32mapply_lora_to_mlp\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \\\\\\n  \u001b[0m\n",
       "\u001b[32mmodel.\u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \\\\\\n  \u001b[0m\n",
       "\u001b[32mmodel.\u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.llama3.lora_llama3_8b\\n \u001b[0m\n",
       "\u001b[32mapply_lora_to_mlp: True\\n    l\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 5:\\nDocument_id:num-2\\nContent: wd`\", \"Use it when you have large gradients and can fit a \u001b[0m\n",
       "\u001b[32mlarge enough batch size, since this is not compatible with ``gradient_accumulation_steps``.\"\\n   \u001b[0m\n",
       "\u001b[32m\":ref:`glossary_cpu_offload`\", \"Offloads optimizer states and \u001b[0m\u001b[32m(\u001b[0m\u001b[32moptionally\u001b[0m\u001b[32m)\u001b[0m\u001b[32m gradients to CPU, and performs optimizer\u001b[0m\n",
       "\u001b[32msteps on CPU. This can be used to significantly reduce GPU memory usage at the cost of CPU RAM and training speed. \u001b[0m\n",
       "\u001b[32mPrioritize using it only if the other techniques are not enough.\"\\n   \":ref:`glossary_lora`\", \"When you want to \u001b[0m\n",
       "\u001b[32msignificantly reduce the number of trainable parameters, saving gradient and optimizer memory during training, and \u001b[0m\n",
       "\u001b[32msignificantly speeding up training. This may reduce training accuracy\"\\n   \":ref:`glossary_qlora`\", \"When you are \u001b[0m\n",
       "\u001b[32mtraining a large model, since quantization will save 1.5 bytes * \u001b[0m\u001b[32m(\u001b[0m\u001b[32m# of model parameters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, at the potential cost of \u001b[0m\n",
       "\u001b[32msome training speed and accuracy.\"\\n   \":ref:`glossary_dora`\", \"a variant of LoRA that may improve model \u001b[0m\n",
       "\u001b[32mperformance at the cost of slightly more memory.\"\\n\\n\\n.. note::\\n\\n  In its current state, this tutorial is \u001b[0m\n",
       "\u001b[32mfocused on single-device optimizations. Check in soon as we update this page\\n  for the latest memory optimization \u001b[0m\n",
       "\u001b[32mfeatures for distributed fine-tuning.\\n\\n.. _glossary_precision:\\n\\n\\nModel Precision\\n---------------\\n\\n*What\\'s \u001b[0m\n",
       "\u001b[32mgoing on here?*\\n\\nWe use the term \"precision\" to refer to the underlying data type used to represent the model and\u001b[0m\n",
       "\u001b[32moptimizer parameters.\\nWe support two data types in torchtune:\\n\\n.. note::\\n\\n  We recommend diving into Sebastian\u001b[0m\n",
       "\u001b[32mRaschka\\'s `blogpost on mixed-precision techniques \u001b[0m\n",
       "\u001b[32m<https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html\u001b[0m\u001b[32m>\u001b[0m\u001b[32m`_\\n  for a deeper understanding of concepts \u001b[0m\n",
       "\u001b[32maround precision and data formats.\\n\\n* ``fp32``, commonly referred to as \"full-precision\", uses 4 bytes per model \u001b[0m\n",
       "\u001b[32mand optimizer parameter.\\n* ``bfloat16``, referred to as \"half-precision\", uses 2 bytes per model and optimizer \u001b[0m\n",
       "\u001b[32mparameter - effectively half\\n  the memory of ``fp32``, and also improves training speed. Generally, if your \u001b[0m\n",
       "\u001b[32mhardware supports training with ``bfloat16``,\\n  we recommend using it - this is the default setting for our \u001b[0m\n",
       "\u001b[32mrecipes.\\n\\n.. note::\\n\\n  Another common paradigm is \"mixed-precision\" training: where model weights are in \u001b[0m\n",
       "\u001b[32m``bfloat16`` \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor ``fp16``\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and optimizer\\n  states are in ``fp32``. Currently,\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtext\u001b[0m=\u001b[32m'END of knowledge_search tool results.\\n'\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this \u001b[0m\n",
       "\u001b[32mquery.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# higher level tool provides packaged results, can span multiple dbs\n",
    "tool_response = client.tool_runtime.rag_tool.query(\n",
    "    content=prompt, vector_db_ids=[vector_db_id]\n",
    ")\n",
    "rich.print(tool_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cac4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryChunksResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chunks</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">' Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates &amp; special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n            \"role\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n        {\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        },\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        },\\n    ]\\n\\nNow, let\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'ora_finetune_single_device --config llama3/8B_qlora_single_device \\\\\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n  model.lora_alpha=64\\n\\n\\nor, by modifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune.models.qlora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"v_proj\"]\\n    lora_rank: 32\\n    lora_alpha: 64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(DoRA)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;https://arxiv.org/abs/2402.09353&gt;`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">How do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``use_dora=True``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scores</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11706648021936417</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10926921665668488</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0655563473701477</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryChunksResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchunks\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank\u001b[0m\u001b[32m<\u001b[0m\u001b[32m<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'token_count'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m512.0\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'document_id'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'num-3'\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mChunk\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mcontent\u001b[0m\u001b[39m=\u001b[0m\u001b[32m' Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates & special \u001b[0m\n",
       "\u001b[32mtokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn \u001b[0m\n",
       "\u001b[32maccompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \u001b[0m\n",
       "\u001b[32m\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n \u001b[0m\n",
       "\u001b[32m\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop\u001b[0m\n",
       "\u001b[32m\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s \u001b[0m\n",
       "\u001b[32mformat this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The\u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to \u001b[0m\n",
       "\u001b[32mindicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, \u001b[0m\n",
       "\u001b[32mMessage\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m <<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'token_count'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m512.0\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'document_id'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'num-0'\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mChunk\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mcontent\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'ora_finetune_single_device --config llama3/8B_qlora_single_device \\\\\\n  \u001b[0m\n",
       "\u001b[32mmodel.\u001b[0m\u001b[32mapply_lora_to_mlp\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\",\"k_proj\",\"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m\\\\\\n  model.\u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m\\n\\n\\nor, by modifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: \u001b[0m\n",
       "\u001b[32mtorchtune.models.qlora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\", \"k_proj\", \u001b[0m\n",
       "\u001b[32m\"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 32\\n    lora_alpha: 64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mDoRA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA \u001b[0m\n",
       "\u001b[32m<https://arxiv.org/abs/2402.09353\u001b[0m\u001b[32m>\u001b[0m\u001b[32m`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing \u001b[0m\n",
       "\u001b[32mthe pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector \u001b[0m\n",
       "\u001b[32mthat adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates \u001b[0m\n",
       "\u001b[32mthe orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude \u001b[0m\n",
       "\u001b[32mparameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! \u001b[0m\n",
       "\u001b[32mHow do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the\u001b[0m\n",
       "\u001b[32msame model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with \u001b[0m\n",
       "\u001b[32m``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use \u001b[0m\n",
       "\u001b[32m:func:`torchtune.models.llama3.lora_llama3_8b` with ``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``:\\n\\n.. code-block:: bash\\n\\n  tune run \u001b[0m\n",
       "\u001b[32mlora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-2'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mscores\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.11706648021936417\u001b[0m, \u001b[1;36m0.10926921665668488\u001b[0m, \u001b[1;36m0.0655563473701477\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can also query the vector db directly\n",
    "db_response = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=prompt,\n",
    ")\n",
    "rich.print(db_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f07682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_context = tool_response.content\n",
    "prompt_context = \"\\n\".join([c.content for c in db_response.chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97855fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'system'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant.'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and (b) the memory constraints of your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n Instruct.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates &amp; special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n            \"role\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n        {\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        },\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        },\\n    ]\\n\\nNow, let\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently\\nora_finetune_single_device --config </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama3/8B_qlora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=32 \\\\\\n  model.lora_alpha=64\\n\\n\\nor, by</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.qlora_llama3_8b\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 32\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(DoRA)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">&lt;https://arxiv.org/abs/2402.09353&gt;`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">How do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">``use_dora=True``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA\\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'system'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'You are a helpful assistant.'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your \u001b[0m\n",
       "\u001b[32mhardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to \u001b[0m\n",
       "\u001b[32mexperiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA \u001b[0m\n",
       "\u001b[32mto Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply \u001b[0m\n",
       "\u001b[32mLoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to\u001b[0m\n",
       "\u001b[32mincrease our max memory,\\nbut as long as we keep :code:`rank\u001b[0m\u001b[32m<\u001b[0m\u001b[32m<embed_dim`, the impact should be relatively \u001b[0m\n",
       "\u001b[32mminor.\\n\\nLet\\'s run this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and\u001b[0m\n",
       "\u001b[32mrank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \u001b[0m\n",
       "\u001b[32m--config llama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between \u001b[0m\n",
       "\u001b[32mthis run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n Instruct.\\n\\n.. \u001b[0m\n",
       "\u001b[32m_prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates & special \u001b[0m\n",
       "\u001b[32mtokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn \u001b[0m\n",
       "\u001b[32maccompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \u001b[0m\n",
       "\u001b[32m\"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n \u001b[0m\n",
       "\u001b[32m\"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all time?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of the most influential hip-hop\u001b[0m\n",
       "\u001b[32m\"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s \u001b[0m\n",
       "\u001b[32mformat this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee how it gets tokenized. The\u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a prompt with flavor text to \u001b[0m\n",
       "\u001b[32mindicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import Llama2ChatTemplate, \u001b[0m\n",
       "\u001b[32mMessage\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m <<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently\\nora_finetune_single_device --config \u001b[0m\n",
       "\u001b[32mllama3/8B_qlora_single_device \\\\\\n  model.\u001b[0m\u001b[32mapply_lora_to_mlp\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m \\\\\\n  \u001b[0m\n",
       "\u001b[32mmodel.\u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\",\"k_proj\",\"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \\\\\\n  model.\u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m\\n\\n\\nor, by\u001b[0m\n",
       "\u001b[32mmodifying a config:\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.qlora_llama3_8b\\n    \u001b[0m\n",
       "\u001b[32mapply_lora_to_mlp: True\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"q_proj\", \"k_proj\", \"v_proj\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 32\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m64\\n\\n.. _glossary_dora:\\n\\nWeight-Decomposed Low-Rank Adaptation \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mDoRA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n*What\\'s going on here?*\\n\\n`DoRA \u001b[0m\n",
       "\u001b[32m<https://arxiv.org/abs/2402.09353\u001b[0m\u001b[32m>\u001b[0m\u001b[32m`_ is another PEFT technique which builds on-top of LoRA by\\nfurther decomposing \u001b[0m\n",
       "\u001b[32mthe pre-trained weights into two components: magnitude and direction. The magnitude component\\nis a scalar vector \u001b[0m\n",
       "\u001b[32mthat adjusts the scale, while the direction component corresponds to the original LoRA decomposition and\\nupdates \u001b[0m\n",
       "\u001b[32mthe orientation of weights.\\n\\nDoRA adds a small overhead to LoRA training due to the addition of the magnitude \u001b[0m\n",
       "\u001b[32mparameter, but it has been shown to\\nimprove the performance of LoRA, particularly at low ranks.\\n\\n*Sounds great! \u001b[0m\n",
       "\u001b[32mHow do I use it?*\\n\\nMuch like LoRA and QLoRA, you can finetune using DoRA with any of our LoRA recipes. We use the\u001b[0m\n",
       "\u001b[32msame model builders for LoRA\\nas we do for DoRA, so you can use the ``lora_`` version of any model builder with \u001b[0m\n",
       "\u001b[32m``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``. For example, to finetune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use \u001b[0m\n",
       "\u001b[32m:func:`torchtune.models.llama3.lora_llama3_8b` with ``\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m``:\\n\\n.. code-block:: bash\\n\\n  tune run \u001b[0m\n",
       "\u001b[32mlora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.\u001b[0m\u001b[32muse_dora\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends \u001b[0m\n",
       "\u001b[32mLoRA\\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "extended_prompt = f\"\"\"\n",
    "Please answer the given query using the context below.\n",
    "\n",
    "QUERY:\n",
    "{prompt}\n",
    "\n",
    "CONTEXT:\n",
    "{prompt_context}\n",
    "\"\"\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "rich.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "526a6848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here are the top 5 topics that were explained:\\n\\n* LoRA (Low-Rank Adaptation)\\n* QLoRA (Quantized</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Low-Rank Adaptation)\\n* DoRA (Weight-Decomposed Low-Rank Adaptation)\\n* Llama2ChatTemplate (prompt template for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structured prompts)\\n* Memory constraints and trade-offs in fine-tuning models with LoRA'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1592.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1677.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Here are the top 5 topics that were explained:\\n\\n* LoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLow-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* QLoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQuantized\u001b[0m\n",
       "\u001b[32mLow-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* DoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWeight-Decomposed Low-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Llama2ChatTemplate \u001b[0m\u001b[32m(\u001b[0m\u001b[32mprompt template for \u001b[0m\n",
       "\u001b[32mstructured prompts\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Memory constraints and trade-offs in fine-tuning models with LoRA'\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1592\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m85\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1677\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=messages,\n",
    "    model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    ")\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "437e8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left as an extra exercise for the reader\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client, \n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools = [\n",
    "        {\n",
    "          \"name\": \"builtin::rag/knowledge_search\",\n",
    "          \"args\" : {\n",
    "            \"vector_db_ids\": [vector_db_id],\n",
    "          }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
