{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d60ee6",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb01a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from pydantic import NonNegativeFloat\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "\n",
    "console = Console()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():  \n",
    "    logger.setLevel(logging.INFO)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b141a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(BaseSettings):\n",
    "    base_url: str\n",
    "\n",
    "    vdb_provider: str\n",
    "    vdb_embedding: str\n",
    "    vdb_embedding_dimension: int\n",
    "    vdb_embedding_window: int\n",
    "\n",
    "    inference_model_id: str\n",
    "    max_tokens: int\n",
    "    temperature: NonNegativeFloat\n",
    "    top_p: float\n",
    "    stream: bool\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8d439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='http://localhost:8321' vdb_provider='milvus' vdb_embedding='all-MiniLM-L6-v2' vdb_embedding_dimension=384 vdb_embedding_window=256 inference_model_id='meta-llama/Llama-3.2-3B-Instruct' max_tokens=4096 temperature=0.0 top_p=0.95 stream=True\n"
     ]
    }
   ],
   "source": [
    "settings = Settings(\n",
    "    base_url=\"http://localhost:8321\",\n",
    "    inference_model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    stream=True,\n",
    "    vdb_provider=\"milvus\",\n",
    "    vdb_embedding=\"all-MiniLM-L6-v2\",\n",
    "    vdb_embedding_dimension=384,\n",
    "    vdb_embedding_window=256,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9171a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n"
     ]
    }
   ],
   "source": [
    "if settings.temperature > 0.0:\n",
    "    strategy = {\n",
    "        \"type\": \"top_p\",\n",
    "        \"temperature\": settings.temperature,\n",
    "        \"top_p\": settings.top_p,\n",
    "    }\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": settings.max_tokens,\n",
    "}\n",
    "\n",
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d42db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n"
     ]
    }
   ],
   "source": [
    "client = LlamaStackClient(base_url=settings.base_url)\n",
    "print(f\"Connected to Llama Stack server @ {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d641",
   "metadata": {},
   "source": [
    "## Validate tools are available in our llama-stack instance\n",
    "\n",
    "When an instance of llama-stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama-stack instance, if you try to register one with the same `toolgroup_id`, llama-stack will throw you an error.\n",
    "\n",
    "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that the `mcp::docling-llamastack` tool is registered, or it will be registered directly from the mcp url.\n",
    "\n",
    "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
    "\n",
    "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`.\n",
    "\n",
    "Make sure to pass the corresponding MCP URL for the server you are trying to register/validate tools for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86dd3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your Llama Stack server is already registered with the following tool groups @ {'mcp::docling-llamastack', 'builtin::rag', 'builtin::websearch'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docling_mcp_url = \"http://localhost:8000/sse\"\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "if \"mcp::docling-llamastack\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::docling-llamastack\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":docling_mcp_url},\n",
    "    )\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "logger.info(f\"Your Llama Stack server is already registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f79fb",
   "metadata": {},
   "source": [
    "## 2. Setup the Ingest + RAG-aware agent\n",
    "- Initialize the collection in the vectordb\n",
    "- Initialize the agent the required tools:\n",
    "    - Docling Ingest will be responsible to take care of instructions like \"Ingest the document https://arxiv.org/pdf/2503.11576\".\n",
    "    - RAG/Knowledge search will respond to user queries by running RAG on the documents ingested in the vectordb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "126fe883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb2fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the vectordb collection to use\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=settings.inference_model_id,\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[\n",
    "        dict(\n",
    "            name=\"mcp::docling-llamastack\",\n",
    "            args={\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "            },\n",
    "        ),\n",
    "        dict(\n",
    "            name=\"builtin::rag/knowledge_search\",\n",
    "            args={\n",
    "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c653cc3",
   "metadata": {},
   "source": [
    "## 3. Executing ingest and RAG queries\n",
    "- For each prompt, initialize a new agent session, execute a turn during which a retrieval call may be requested, and output the reply received from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "374d7929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Ingest the document </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://arxiv.org/pdf/2503.11576</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Ingest the document \u001b[0m\u001b[4;36mhttps://arxiv.org/pdf/2503.11576\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33ming\u001b[0m\u001b[33mest\u001b[0m\u001b[33m_document\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_vect\u001b[0m\u001b[33mord\u001b[0m\u001b[33mb\u001b[0m\u001b[33m(source\u001b[0m\u001b[33m='\u001b[0m\u001b[33mhttps\u001b[0m\u001b[33m://\u001b[0m\u001b[33mar\u001b[0m\u001b[33mxiv\u001b[0m\u001b[33m.org\u001b[0m\u001b[33m/pdf\u001b[0m\u001b[33m/\u001b[0m\u001b[33m250\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m115\u001b[0m\u001b[33m76\u001b[0m\u001b[33m',\u001b[0m\u001b[33m vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m='\u001b[0m\u001b[33myour\u001b[0m\u001b[33m_vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Args:{'source': 'https://arxiv.org/pdf/2503.11576', 'vector_db_id': 'your_vector_db_id'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Response:{\"type\":\"text\",\"text\":\"2503.11576v1.pdf\",\"annotations\":null}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mWhat\u001b[0m\u001b[33m is\u001b[0m\u001b[33m the\u001b[0m\u001b[33m metadata\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ing\u001b[0m\u001b[33mested\u001b[0m\u001b[33m document\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msystem\u001b[0m\u001b[33m vs\u001b[0m\u001b[33m human\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m of\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'system vs human analysis of layout'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTo illustrate and compare the visual grounding capabilities of SmolDocling, a set of sample predictions with SmolDocling and Qwen2.5-VL from DocLayNet [70] is shown in Table 7. Note that the element location results are independent from the correct reproduction of document content and structure.\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text=\"Result 2:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text=\"Result 3:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text='Result 4:\\nDocument_id:17402\\nContent: 5.2 Quantitative Results\\nTo enable valid cross-model comparisons with SmolDocling, we addressed multiple dataset and harmonization challenges:\\n· Public evaluation datasets lack comprehensive multi-task annotations with diverse layouts. We augmented DocLayNet [70] with text content and reading-order to enable evaluation of both layout analysis and text recognition. Table structure, code listing, formula and chart recognition are evaluated on specific datasets.\\n· Different models each follow their own conventions for output markup (e.g., Markdown, HTML, DocTags) and produce annotations which do not fully align in semantics. For example, SmolDocling is trained to produce 14 distinct layout-class tags via DocTags , while Qwen2.5-VL combines standard HTML tags and a few explicit class attributes with only partially compatible definitions. GOT and Nougat output Markdown, which can express paragraphs, lists and tables without location annotation.\\n- · Input resolution of page images can affect model performance, but is rarely specified in literature. We standardized at 144 DPI (dots per inch), which reproduces enough detail to be perfectly legible by humans while keeping resource demand for inference reasonable.\\nText Recognition (OCR). We first evaluate text recognition accuracy of SmolDocling, compared to Nougat, GOT and Qwen2.5-VL (Table 2), to verify accurate character transcription and reading order. The text-similarity metrics, adopted from [12] require plain formatting without non-textual elements. For different tasks, we conduct evaluations on diverse content types: full-page documents (from DocLayNet test set) in Markdown format, code snippets (from our SynthCodeNet dataset) in plaintext, and equations (from Im2Latex-230k [21]) in LaTeX format. For Qwen2.5-VL and SmolDocling, we convert HTML or DocTags output to Markdown while preserving formatting where possible.\\n', type='text'), TextContentItem(text='Result 5:\\nDocument_id:17402\\nContent: 1 Introduction\\nFor decades, converting complex digital documents into a structured, machine-processable format has been a significant technical challenge. This challenge primarily stems from the substantial variability in document layouts and styles, as well as the inherently opaque nature of the widely used PDF format, which is optimized for printing rather than semantic parsing. Intricate layout styles and visually challenging elements such as forms, tables, and complex charts can significantly impact the reading order and general understanding of documents. These problems have driven extensive research and development across multiple domains of computer science. On one hand, sophisticated ensemble systems emerged, which decompose the conversion problem into several sub-tasks (e.g. OCR, layout analysis, table structure recognition, classification) and tackle each sub-task independently. Although such systems can achieve high-quality results for many document types while maintaining relatively low computational demands, they are often difficult to tune and generalize.\\nOn the other hand, in the recent past, great interest has developed around large foundational multimodal models that can solve the whole conversion task in one shot, while simultaneously offering flexible querying and parametrization through prompts. This approach has been made possible by the advent of multimodal pre-training of large visionlanguage models (LVLMs), which opened up a vast array of opportunities for leveraging diverse data sources, including PDF documents. However, the literature on this topic highlights a significant gap in the availability of high-quality and open-access datasets suitable for training robust multi-modal models for the task of document understanding. Furthermore, relying on LVLMs may introduce common issues associated with such models, including hallucinations and the use of significant computational resources, making them impractical from both a quality and cost perspective.\\nFigure 1: SmolDocling/SmolVLM architecture. SmolDocling converts images of document pages to DocTags sequences. First, input images are encoded using a vision encoder and reshaped via projection and pooling. Then, the projected embeddings are concatenated with the text embeddings of the user prompt, possibly with interleaving. Finally, the sequence is used by an LLM to autoregressively predict the DocTags sequence.\\n', type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"system vs human analysis of layout\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m their\u001b[0m\u001b[33m strengths\u001b[0m\u001b[33m and\u001b[0m\u001b[33m weaknesses\u001b[0m\u001b[33m when\u001b[0m\u001b[33m analyzing\u001b[0m\u001b[33m layouts\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m are\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m in\u001b[0m\u001b[33m terms\u001b[0m\u001b[33m of\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mSm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m is\u001b[0m\u001b[33m able\u001b[0m\u001b[33m to\u001b[0m\u001b[33m handle\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-column\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m has\u001b[0m\u001b[33m some\u001b[0m\u001b[33m recall\u001b[0m\u001b[33m errors\u001b[0m\u001b[33m in\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m also\u001b[0m\u001b[33m struggles\u001b[0m\u001b[33m with\u001b[0m\u001b[33m portrait\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m that\u001b[0m\u001b[33m have\u001b[0m\u001b[33m colorful\u001b[0m\u001b[33m elements\u001b[0m\u001b[33m and\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m backgrounds\u001b[0m\u001b[33m.\u001b[0m\u001b[33m On\u001b[0m\u001b[33m the\u001b[0m\u001b[33m other\u001b[0m\u001b[33m hand\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m handles\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m conf\u001b[0m\u001b[33muses\u001b[0m\u001b[33m tables\u001b[0m\u001b[33m and\u001b[0m\u001b[33m pictures\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mQ\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m has\u001b[0m\u001b[33m poor\u001b[0m\u001b[33m bounding\u001b[0m\u001b[33m box\u001b[0m\u001b[33m recall\u001b[0m\u001b[33m on\u001b[0m\u001b[33m manual\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m with\u001b[0m\u001b[33m terminal\u001b[0m\u001b[33m output\u001b[0m\u001b[33m and\u001b[0m\u001b[33m is\u001b[0m\u001b[33m confused\u001b[0m\u001b[33m between\u001b[0m\u001b[33m tables\u001b[0m\u001b[33m and\u001b[0m\u001b[33m pictures\u001b[0m\u001b[33m.\u001b[0m\u001b[33m However\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m reconstruct\u001b[0m\u001b[33ms\u001b[0m\u001b[33m equations\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m with\u001b[0m\u001b[33m different\u001b[0m\u001b[33m annotation\u001b[0m\u001b[33m conventions\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m results\u001b[0m\u001b[33m suggest\u001b[0m\u001b[33m that\u001b[0m\u001b[33m while\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m their\u001b[0m\u001b[33m strengths\u001b[0m\u001b[33m and\u001b[0m\u001b[33m weaknesses\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m seems\u001b[0m\u001b[33m to\u001b[0m\u001b[33m perform\u001b[0m\u001b[33m better\u001b[0m\u001b[33m in\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-column\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m and\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m,\u001b[0m\u001b[33m while\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m performs\u001b[0m\u001b[33m better\u001b[0m\u001b[33m in\u001b[0m\u001b[33m reconstruct\u001b[0m\u001b[33ming\u001b[0m\u001b[33m equations\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIt\u001b[0m\u001b[33m's\u001b[0m\u001b[33m also\u001b[0m\u001b[33m worth\u001b[0m\u001b[33m noting\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m results\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m challenges\u001b[0m\u001b[33m of\u001b[0m\u001b[33m comparing\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m as\u001b[0m\u001b[33m they\u001b[0m\u001b[33m use\u001b[0m\u001b[33m different\u001b[0m\u001b[33m conventions\u001b[0m\u001b[33m for\u001b[0m\u001b[33m output\u001b[0m\u001b[33m markup\u001b[0m\u001b[33m and\u001b[0m\u001b[33m may\u001b[0m\u001b[33m have\u001b[0m\u001b[33m varying\u001b[0m\u001b[33m levels\u001b[0m\u001b[33m of\u001b[0m\u001b[33m detail\u001b[0m\u001b[33m in\u001b[0m\u001b[33m their\u001b[0m\u001b[33m annotations\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mOverall\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m room\u001b[0m\u001b[33m for\u001b[0m\u001b[33m improvement\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m seems\u001b[0m\u001b[33m to\u001b[0m\u001b[33m be\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m robust\u001b[0m\u001b[33m performer\u001b[0m\u001b[33m in\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Ingest the document https://arxiv.org/pdf/2503.11576\",\n",
    "    \"Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    console.print(f\"\\n[cyan]User> {prompt}[/cyan]\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed08e3",
   "metadata": {},
   "source": [
    "## 4. Choose the documents and answer the question\n",
    "\n",
    "In this section we add one more tool which is retrieving the documents matching some metadata, e.g. the publication year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36de03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the vectordb collection to use\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n",
    "# define the same agent (with a new vectordb)\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=settings.inference_model_id,\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[\n",
    "        dict(\n",
    "            name=\"mcp::docling-llamastack\",\n",
    "            args={\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "            },\n",
    "        ),\n",
    "        dict(\n",
    "            name=\"builtin::rag/knowledge_search\",\n",
    "            args={\n",
    "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdacc19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Find publications of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Find publications of \u001b[0m\u001b[1;36m2022\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33msearch\u001b[0m\u001b[33m_files\u001b[0m\u001b[33m_metadata\u001b[0m\u001b[33m(year\u001b[0m\u001b[33m='\u001b[0m\u001b[33m202\u001b[0m\u001b[33m2\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:search_files_metadata Args:{'year': '2022'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:search_files_metadata Response:{\"type\":\"text\",\"text\":\"file:///Users/dol/projects/aimasterclass-workshop/ai-masterclass-2025/unit2/data/2022/IBM/2206.01062.pdf\",\"annotations\":null}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mIt\u001b[0m\u001b[33m seems\u001b[0m\u001b[33m like\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m_files\u001b[0m\u001b[33m_metadata\u001b[0m\u001b[33m function\u001b[0m\u001b[33m returned\u001b[0m\u001b[33m a\u001b[0m\u001b[33m single\u001b[0m\u001b[33m file\u001b[0m\u001b[33m.\u001b[0m\u001b[33m If\u001b[0m\u001b[33m you\u001b[0m\u001b[33m want\u001b[0m\u001b[33m to\u001b[0m\u001b[33m find\u001b[0m\u001b[33m all\u001b[0m\u001b[33m publications\u001b[0m\u001b[33m from\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m2\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m following\u001b[0m\u001b[33m command\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m[\u001b[0m\u001b[33msearch\u001b[0m\u001b[33m_files\u001b[0m\u001b[33m_metadata\u001b[0m\u001b[33m(year\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m202\u001b[0m\u001b[33m2\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Ingest the selected documents</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Ingest the selected documents\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33ming\u001b[0m\u001b[33mest\u001b[0m\u001b[33m_document\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_vect\u001b[0m\u001b[33mord\u001b[0m\u001b[33mb\u001b[0m\u001b[33m(source\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mfile\u001b[0m\u001b[33m:///\u001b[0m\u001b[33mUsers\u001b[0m\u001b[33m/d\u001b[0m\u001b[33mol\u001b[0m\u001b[33m/projects\u001b[0m\u001b[33m/\u001b[0m\u001b[33maim\u001b[0m\u001b[33master\u001b[0m\u001b[33mclass\u001b[0m\u001b[33m-work\u001b[0m\u001b[33mshop\u001b[0m\u001b[33m/\u001b[0m\u001b[33mai\u001b[0m\u001b[33m-master\u001b[0m\u001b[33mclass\u001b[0m\u001b[33m-\u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m/unit\u001b[0m\u001b[33m2\u001b[0m\u001b[33m/data\u001b[0m\u001b[33m/\u001b[0m\u001b[33m202\u001b[0m\u001b[33m2\u001b[0m\u001b[33m/\u001b[0m\u001b[33mIBM\u001b[0m\u001b[33m/\u001b[0m\u001b[33m220\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m010\u001b[0m\u001b[33m62\u001b[0m\u001b[33m.pdf\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33myour\u001b[0m\u001b[33m_vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Args:{'source': 'file:///Users/dol/projects/aimasterclass-workshop/ai-masterclass-2025/unit2/data/2022/IBM/2206.01062.pdf', 'vector_db_id': 'your_vector_db_id'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Response:{\"type\":\"text\",\"text\":\"2206.01062.pdf\",\"annotations\":null}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mPlease\u001b[0m\u001b[33m replace\u001b[0m\u001b[33m \"\u001b[0m\u001b[33myour\u001b[0m\u001b[33m_vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m actual\u001b[0m\u001b[33m ID\u001b[0m\u001b[33m of\u001b[0m\u001b[33m your\u001b[0m\u001b[33m vector\u001b[0m\u001b[33m database\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mAlso\u001b[0m\u001b[33m,\u001b[0m\u001b[33m note\u001b[0m\u001b[33m that\u001b[0m\u001b[33m I\u001b[0m\u001b[33m assume\u001b[0m\u001b[33m you\u001b[0m\u001b[33m want\u001b[0m\u001b[33m to\u001b[0m\u001b[33m ingest\u001b[0m\u001b[33m only\u001b[0m\u001b[33m one\u001b[0m\u001b[33m document\u001b[0m\u001b[33m at\u001b[0m\u001b[33m a\u001b[0m\u001b[33m time\u001b[0m\u001b[33m.\u001b[0m\u001b[33m If\u001b[0m\u001b[33m you\u001b[0m\u001b[33m have\u001b[0m\u001b[33m multiple\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m selected\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m'll\u001b[0m\u001b[33m need\u001b[0m\u001b[33m to\u001b[0m\u001b[33m call\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ingest\u001b[0m\u001b[33m_document\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_vect\u001b[0m\u001b[33mord\u001b[0m\u001b[33mb\u001b[0m\u001b[33m function\u001b[0m\u001b[33m for\u001b[0m\u001b[33m each\u001b[0m\u001b[33m one\u001b[0m\u001b[33m separately\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msystem\u001b[0m\u001b[33m vs\u001b[0m\u001b[33m human\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m of\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'system vs human analysis of layout'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:71562\\nContent: DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\\n', type='text'), TextContentItem(text='Result 2:\\nDocument_id:71562\\nContent: Example Predictions\\nTo conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.\\n', type='text'), TextContentItem(text=\"Result 3:\\nDocument_id:71562\\nContent: CCS CONCEPTS\\n· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\n© 2022 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-9385-0/22/08.\\nhttps://doi.org/10.1145/3534678.3539043\\nFigure 1: Four examples of complex page layouts across different document categories\\n\", type='text'), TextContentItem(text='Result 4:\\nDocument_id:71562\\nContent: 1 INTRODUCTION\\nIn this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:\\n(1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\\n(2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.\\n(3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.\\n(4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.\\n1 https://developer.ibm.com/exchanges/data/all/doclaynet\\nThis enables experimentation with annotation uncertainty and quality control analysis.\\n- (5) Pre-defined Train-, Test- & Validation-set : Like DocBank, we provide fixed train-, test- & validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.\\nAll aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.\\nIn Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.\\n', type='text'), TextContentItem(text=\"Result 5:\\nDocument_id:71562\\nContent: 3 THE DOCLAYNET DATASET\\nDespite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"system vs human analysis of layout\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m system\u001b[0m\u001b[33m (\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m)\u001b[0m\u001b[33m has\u001b[0m\u001b[33m achieved\u001b[0m\u001b[33m robust\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m recovery\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m other\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m baseline\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m numbers\u001b[0m\u001b[33m for\u001b[0m\u001b[33m object\u001b[0m\u001b[33m detection\u001b[0m\u001b[33m models\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m.\u001b[0m\u001b[33m However\u001b[0m\u001b[33m,\u001b[0m\u001b[33m human\u001b[0m\u001b[33m annotation\u001b[0m\u001b[33m has\u001b[0m\u001b[33m several\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m over\u001b[0m\u001b[33m automated\u001b[0m\u001b[33m ground\u001b[0m\u001b[33m-tr\u001b[0m\u001b[33muth\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ability\u001b[0m\u001b[33m to\u001b[0m\u001b[33m annotate\u001b[0m\u001b[33m any\u001b[0m\u001b[33m type\u001b[0m\u001b[33m of\u001b[0m\u001b[33m document\u001b[0m\u001b[33m without\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m a\u001b[0m\u001b[33m program\u001b[0m\u001b[33mmatic\u001b[0m\u001b[33m source\u001b[0m\u001b[33m and\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m natural\u001b[0m\u001b[33m interpretation\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m page\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m system\u001b[0m\u001b[33m's\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m is\u001b[0m\u001b[33m impacted\u001b[0m\u001b[33m by\u001b[0m\u001b[33m varying\u001b[0m\u001b[33m the\u001b[0m\u001b[33m dataset\u001b[0m\u001b[33m size\u001b[0m\u001b[33m,\u001b[0m\u001b[33m reducing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m label\u001b[0m\u001b[33m set\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m modifying\u001b[0m\u001b[33m the\u001b[0m\u001b[33m train\u001b[0m\u001b[33m/test\u001b[0m\u001b[33m-split\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m results\u001b[0m\u001b[33m also\u001b[0m\u001b[33m show\u001b[0m\u001b[33m that\u001b[0m\u001b[33m a\u001b[0m\u001b[33m model\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m overall\u001b[0m\u001b[33m more\u001b[0m\u001b[33m robust\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m recovery\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m models\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m Pub\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mBank\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIn\u001b[0m\u001b[33m summary\u001b[0m\u001b[33m,\u001b[0m\u001b[33m while\u001b[0m\u001b[33m the\u001b[0m\u001b[33m system\u001b[0m\u001b[33m has\u001b[0m\u001b[33m its\u001b[0m\u001b[33m strengths\u001b[0m\u001b[33m and\u001b[0m\u001b[33m weaknesses\u001b[0m\u001b[33m,\u001b[0m\u001b[33m human\u001b[0m\u001b[33m annotation\u001b[0m\u001b[33m is\u001b[0m\u001b[33m still\u001b[0m\u001b[33m essential\u001b[0m\u001b[33m for\u001b[0m\u001b[33m achieving\u001b[0m\u001b[33m high\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m and\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m a\u001b[0m\u001b[33m natural\u001b[0m\u001b[33m interpretation\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m page\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Find publications of 2022\",\n",
    "    \"Ingest the selected documents\",\n",
    "    # \"How does the system compare to humans when analyzing the layout?\",\n",
    "    \"Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\",\n",
    "]\n",
    "\n",
    "session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\")\n",
    "for prompt in queries:\n",
    "    console.print(f\"\\n[cyan]User> {prompt}[/cyan]\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=settings.stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa41de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4c772",
   "metadata": {},
   "source": [
    "## ReAct Agent\n",
    "\n",
    "In the following section we use the reasoning agent `ReActAgent`. In this scenario, the model orchestrator the tools execution is reasoning on the sequence of tools to be executed in order to perform the task.\n",
    "\n",
    "This allows to have a single user query which triggers multiple independent steps, e.g.\n",
    "\n",
    "1. Search for the files matching the metadata (e.g. which publication year)\n",
    "2. Ingest the documents\n",
    "3. Run a search query on the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=settings.inference_model_id,\n",
    "            tools=[\n",
    "                dict(\n",
    "                    name=\"mcp::docling-llamastack\",\n",
    "                    args={\n",
    "                        \"vector_db_id\": vector_db_id,\n",
    "                    },\n",
    "                ),\n",
    "                dict(\n",
    "                    name=\"builtin::rag/knowledge_search\",\n",
    "                    args={\n",
    "                        \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "                    },\n",
    "                )\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    \"I would like to ingest publications of 2025 and understand how does SmolDocling system compare to humans when analyzing the layout?\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    console.print(f\"[cyan]Processing user query: {prompt}[/cyan]\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream\n",
    "    )\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e85043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
